(C) Copyright 2006 John Graley
Available under the terms of the BSD license. 

Objectives of the program transformation


Intro
-----

Technically, any transformation that converts C++ to Verilog whilst preserving functionality would satisfy this project's self-defined remit. And since figuring out such program transformations is hard (see the link farm of the project web page at inferno-synth.sourceforge.net) it is tempting to go for anything that will work.

However on inspecting the two languages and real programs/systems created in them, some correspondences are obvious, and some differences (or incompatibility) also make themselves known. We use some of these observations to guide the transformation process and thereby keep it sane and retain at least some vague resemblance to human-generated code.


Objects correspond to modules
-----------------------------

The C++ concept of the object (instance of a class) corresponds closely to the Verilog module. Each encapsulates data storage and functionality, and presents an interface to other object/modules. They may be nested in a tree structure that emanates from the global scope and the languages limit access to child object/modules from outside.

A concept of locality is attributable to both: in the case of objects this locality simplifies design/maintainence/debugging and may improve cachability of the resultant program. Synthesis turns a verilog module into a physically localised area within the layout; speed and power advantages are to be gained from shorter wires and simpler routing.

Both structural forms relate indirectly to concurrency by delineating the extent to which different models will be used. Concurrency between objects and modules is typically a "loose" style in which event/timing interdependencies are the minimum required for the system to function. In C++ we have threads where most objects do not "own" more than one thread. In Verilog we see busses between modules where the modules cannot know when the next request will arrive. In both languages, explicit synchronisation is required when interactions do occur in loosely concurrent environments.

Concurrency occurs within Verilog modules too, but usually in a much more tightly controlled fashion whereby shared implementation details remove the need for explicit synchronisation. An equivalant in software would be a "PAR" block as in the Occam language.

The similarities are such that it appears advisable to map C++ objects directly to Verilog modules on a 1:1 basis. 

This implies that multiple objects of the same type will duplicate the functionality which may be seen as redundant. However, this duplication is needed of obtain the level of locality and concurrency found in typical hardware designs.

It also implies that the existence of all objects must be inferred at translation time and therefore all objects must be static. We are forced therefore to build explicit object pools whenever "new" is used and the size of the pools must be decided in advance.


Corresponding patterns
----------------------

Certain patterns may be identified in software programs which have analogies in hardware design.

One is the interacting agents pattern. This resembles a group of relatively autonomous entities that need only interact at specific times and pass only a limited and strictly defined amount of data to each other. They are often candidates for independent development/testing.

They are found at the top level of a program and work in a concurrent style even if actual concurrency is not being used (it can normally be added easily). The same pattern may be found in the higher levels of a hardware design - in this case maximum parallelism is the accepted norm. 

This interacting agents pattern should be preserved where found (basically at the inter-object level) and the interaction events and associated data should be preserved.

Another important pattern is the state machine. In hardware the state machine dominates when sequential control is required. It can usually be found controlling the rest of the logic in a given module. 

Software enjoys (or suffers) a choice of options in terms of implementing sequential control. The main options are explicit state machines that resemble their hardware cousins or simply running a sequential algorithm that retains the context of execution until the task is complete. The retained context issue is discussed later, but the duality between software and hardware state machines is too important to ignore. Such state machines ought to be able to correspond one to one.

The last remaining pattern correspondence is simply the existence of atomic, or effectively atomic blocks of functionality at the very lowest level of a program. Here, we argue that if a given algorithm may be represented by a sufficiently small truth table relating inputs to outputs then it is trivially representable in software or hardware and there is no need to break up or otherwise significantly alter such blocks. 


Pointers
--------

Pointers play an important role in defining the communication structure in the context of the interacting agents pattern given above. In this role, we choose the pointer as the C++ construct that will ultimately become the fully-featured communications infrastructure required for this pattern's hardware realisation.

Pointers have very flexible run-time behaviour, and the closest correspondence that a hardware communications infrastructure can offer is routing (which means demultiplexing under a protocol). Therefore, a pointer's run-time value must somehow correspond with the control input to routers.

A helpful intermediate way of thinking about pointers is as "channels" in the sense used in a number of papers on the subject. We therefore aim to understand the correspondence between pointers, channels and physical bus/router/arbiter systems and transform them accordingly.


Recursion
---------

Recursion is a commonly used software activity which has no simple alias in hardware, probably because it has only limited locality (because its working data is a stack whose size is proportional to maximum recursion depth, which is an algorithmic parameter). 

I believe the non-locality must be accepted if (non-trivial) recursive code is to be translated and since there is no clean (or commonly used) hardware technique for this we resort to the methods in "Minimal recreation of computer-like hardware" below.


No central memory or address demultiplexer
------------------------------------------

Though this has been touched on in earlier sections, it must be remembered that a hand-written hardware system does not have to revolve around a central RAM. If the system happens to include a conventional microprocessor, then there will probably be RAM, and hardware may access that RAM. But RAM is understood to be slow in comparison to the types of speeds expected of dedicated hardware and thus a bottleneck.

We therefore have to address any and all aspects of C++ programs that imply dependence on RAM. We obviously need storage, but that will be distributed across the system in objects and hence local to the functionality that needs to use it.

The real problem with having no central RAM isn't that we won't have enough storage elements (we obviously have to have enough storage for the program's algorithm) but the fact that we don't have the large multiplexer/demultiplexer found in any RAM chip that allows any part of the RAM to be reached via a simple binary number (the address).
 
This basically prohibits the use of pointers as flat addresses which means in practice we may not be able to support certain types of pointer cases.


Minimal recreation of computer-like hardware
--------------------------------------------

It should be understood that one trivial solution to the problem of producing hardware that implements a software program is to simply provide a conventional computer that includes (a compiled version of) the software program in a ROM.

This solution does not advance the state of the art; in spite of decades of development it is still power-hungry, slow, has poor locality and limited concurrency. The mappings described here improve on that for "easy" cases in which the software language corresponds well with hardware capabilities.

However, when faced with more difficult problems it is helpful to consider that conventional computers nonetheless *always* offer an answer to the problem of how to implement some software idiom in hardware.

In order to exploit this and still avoid the drawbacks of conventional computers, we consider minimal forms that include the features we need and avoid the bloat of a full computer. This is best achieved by (a) minimising the problem as much as possible by stripping away easily solved parts of it and then (b) picking out exactly the features that a computer has that we need to solve the remaining stripped-down problem.

For example, the problem of the retained context (mentioned above) is to note that the only difficult aspect is that the software's execution needs to be spread over time, and that is exactly what a conventional computer does by executing one machine instruction at a time (or a small number at a time).

The minimal elements that a computer uses to achieve this are: (1) facilities for performing each atomic action that will be needed, (2) a read-only record of which action to do at which step in the process and (3) a read-write variable to hold the current step. Looking at a conventional computer from this point of view makes it resemble a traditional state machine. We now know that we may solve the problem by creating a state machine.

A further example is recursion. A traditional state machine seems unable to support recursion. So we ask: what is it that a conventional computer has that a traditional state machine does not and that lets it recurse? The answer is a stack, and a stack pointer. Since that stack stores the program counter, our stack must now store our state variable and this fundamentally changes the state machine, but it can now recurse.

The minimal recreation of computer-like hardware is not a guiding concept for the project - direct correspondences are the more powerful technique - but it is a useful fall-back. Note that the computer-like devices are always seen as the *internals* of a given object. Inter-object interactions more closely resemble a network of very simple conventional computers.


@todo find links on contexts, coroutines, cooperative tasking etc
