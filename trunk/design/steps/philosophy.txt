(C) Copyright 2006 John Graley
Available under the terms of the BSD license. 

Philospohy of the program transformation


Intro
-----

Technically, any transformation that converts C++ to Verilog whilst preserving functionality would satisfy this project's self-defined remit. And since figuring out such program transformations is hard (see the link farm at the project web page at inferno-synth.sourceforge.net) it is tempting to go for anything that will work.

However on inspecting the two languages and real programs/systems created in them, some correspondences are obvious, and some differences (or incompatibility) also make themselves known. We use some of these observations to guide the transformation process and thereby keep it sane and retain at least some vague resemblance to human-generated code.

Finally we devise a handy and reassuring intellectual "crutch" based on the most obvious yet most commonly overlooked difference between a hardware-based system and a software-based system: the presence of a computer in the latter.


Objects correspond to modules
-----------------------------

The C++ concept of the object (instance of a class) corresponds closely to the Verilog module. Each encapsulates data storage and functionality, and presents an interface to other objects/modules. They may be nested in a tree structure that emanates from the global scope and the languages limit access to child object/modules from external objects/modules.

Locality is attributable to both: in the case of objects this locality simplifies design/maintainence/debugging and may improve cachability of the resultant program. Synthesis turns a verilog module into a physically localised area within the layout; speed and power advantages are to be gained from shorter wires and simpler routing.

Both structural forms relate indirectly to concurrency by delineating the extent to which different concurrency models will be used. Concurrency between objects and modules is typically a "loose" style in which event/timing interdependencies are the minimum required for the system to function. In C++ we have threads where most objects do not "own" more than one thread, so threading may be viewed as inter-object. In Verilog we see busses between modules where the modules cannot know when the next request will arrive. In both languages, explicit synchronisation is required when interactions do occur in loosely concurrent environments.

Concurrency occurs within Verilog modules too, but usually in a much more tightly controlled fashion whereby shared implementation details remove the need for explicit synchronisation. An equivalant in software would be a "PAR" block as found in the Occam language.

The similarities are such that it appears advisable to map C++ objects directly to Verilog modules on a 1:1 basis. 

This implies that multiple objects of the same type will duplicate functionality which may be seen as redundant. However, this duplication is needed of obtain the level of locality and concurrency found in typical hardware designs.

It also implies that the existence of all objects must be inferred at translation time and therefore all objects must be static. We are forced therefore to build explicit object pools whenever "new" is used and the size of the pools must be decided in advance.


Corresponding patterns
----------------------

Certain patterns may be identified in software programs which have analogies in hardware design.

One is the interacting agents pattern. This resembles a group of relatively autonomous entities that need only interact at specific times and pass only a limited and strictly defined amount of data to each other. They are often candidates for independent development/testing.

They are found at the top level of a program and work in a concurrent style even if actual concurrency is not being used (it can normally be added easily). The same pattern may be found in the higher levels of a hardware design - in this case maximum parallelism is the accepted norm. 

This interacting agents pattern should be preserved where found (basically at the inter-object level) and the interaction events and associated data should be preserved.

Another important pattern is the state machine. In hardware the state machine dominates when sequential control is required. It can usually be found controlling the rest of the logic in a given module. 

Software enjoys (or suffers) a choice of options in terms of implementing sequential control. The main options are explicit state machines that resemble their hardware cousins or simply running a sequential algorithm that retains the context of execution until the task is complete. The retained context issue is discussed later, but the duality between software and hardware state machines is too important to ignore. Such state machines ought to be able to correspond one-to-one.

The last remaining pattern correspondence is simply the existence of atomic, or effectively atomic blocks of functionality at the very lowest level of a program. Here, we argue that if a given algorithm may be represented by a sufficiently small truth table relating inputs to outputs then it is trivially representable in software or hardware and there is no need to break up or otherwise significantly alter such blocks. 


Pointers
--------

Pointers play an important role in defining the communication structure in the context of the interacting agents pattern given above. In this role, we choose the pointer as the C++ construct that will ultimately become the fully-featured communications infrastructure required for this pattern's hardware realisation.

Pointers have very flexible run-time behaviour, and the closest correspondence that a hardware communications infrastructure can offer is routing (which means demultiplexing under a protocol). Therefore, a pointer's run-time value must somehow correspond with the control input to routers.

A helpful intermediate way of thinking about pointers is as "channels" in the sense used in a number of papers on the subject. We therefore aim to understand the correspondence between pointers, channels and physical bus/router/arbiter systems and transform them accordingly.


Recursion
---------

Recursion is a commonly used software activity which has no simple alias in hardware, probably because it has only limited locality (because its working data are a stack whose size is proportional to maximum recursion depth (an algorithmic parameter)). 

I believe the non-locality must be accepted if (non-trivial) recursive code is to be translated and since there is no clean (or commonly used) hardware technique for this we resort to the methods in the "Minimal recreation of computer-like hardware" section below.


No central memory or address demultiplexer
------------------------------------------

Though this has been touched on in earlier sections, it must be remembered that a hand-written hardware system does not have to revolve around a central RAM. If the system happens to include a conventional microprocessor, then there will probably be RAM, and hardware may access that RAM. But RAM is understood to be slow in comparison to the types of speeds expected of dedicated hardware and thus a bottleneck.

We therefore have to address any and all aspects of C++ programs that imply dependence on RAM. We obviously need storage, but that will be distributed across the system in objects and hence local to the functionality that needs to use it.

The real problem with having no central RAM isn't that we won't have enough storage elements (we obviously have to have enough storage for the program's algorithm) but the fact that we don't have the large multiplexer/demultiplexer found in any RAM chip that allows any part of the RAM to be reached via a simple binary number (the address).
 
This basically prohibits the use of pointers as flat addresses which means in practice we may not be able to support certain types of pointer casts.


Minimal recreation of computer-like hardware
--------------------------------------------

It should be understood that one trivial solution to the problem of producing hardware that implements a software program is to simply provide a conventional computer that includes (a compiled version of) the software program in a ROM.

This solution does not advance the state of the art; in spite of decades of development it is still power-hungry, slow, has poor locality and limited concurrency. The mappings described here improve on that for "easy" cases in which the software language corresponds well with existing hardware styles.

However, when faced with more difficult problems it is helpful to consider that conventional computers nonetheless *always* offer an answer to the problem of how to implement some software idiom in hardware.

In order to exploit this and still avoid the drawbacks of conventional computers, we consider minimal forms that include the features we need and avoid the bloat of a full computer. This is best achieved by (a) minimising the problem as much as possible by stripping away easily solved parts of it and then (b) picking out exactly the features that a computer has and that we require to solve the remaining problem.

For example, the problem of the retained context (mentioned above) is to note that the only difficult aspect is that the software's execution needs to be spread over time, and that is exactly what a conventional computer does by executing one machine instruction at a time (or a small number at a time).

The minimal elements that a computer uses to achieve this are: (1) facilities for performing each atomic action that will be needed, (2) a read-only record of which action to do at which step in the process and (3) a read-write variable to hold the current step. Looking at a conventional computer from this point of view makes it resemble a traditional state machine. We now know that we may solve the problem by creating a new state machine. This process is called "stating out" in this project's lexicon.

A further example is recursion. A traditional state machine seems unable to support recursion. So we ask: what is it that a conventional computer has that a traditional state machine does not and that lets it recurse? The answer is a stack, and a stack pointer. Since that stack stores the program counter, our stack must now store our state variable and this fundamentally changes the state machine, but it can now recurse.


Limits on computer re-creation
------------------------------

We choose not to apply the approach of partially building a computer (as described above) to all levels of the application program. Even though our input program may have been constructed to run on a single, sequential computer we nonetheless choose to restrict the recreation of computer-like solutions to the lower levels of the system where we see state machines, sequential algorithms and atomic blocks. At the higher levels, where we see 
interacting agents and concurrency we instead choose a model resembling a network of computers that intercat through a protocol.

This approach is preferred for two reasons:
- It supports greater parallelism 
- It more closely resembles actual hardware designs
- It may map to the programmer's intentions more precisely

Only the last of these is contraversial. We are justifying moving *away* from a single-computer solution that resembles the original platform for which the original program was written on the basis that the programmer really wanted to construct a more parallel, interacting agents, pattern at the highest level of his or her program.

I believe something like this is often, indeed typically, the case. Whether designe4d or evolved, software programs inevitably develop sufficient complexity in their sequential interactions that some sort of guiding structure is required to manage the complexity of the interactions between different parts. The public interfaces of C++ classes are an example of this. The member function prototypes explicitly define parameters and return values, but a non-trivial class usually also specifies the required sequence of calls to its member functions eg construct-init-modifiers/accessors-destruct. Such a specification may be seen as a protocol, and its presence shows that the program has been segregated into seperate parts which may function independently. 

The important point here is that once the programmer has commited to segregating the program based on this kind of interface, we are no longer required to preserve any coherency between the implementations on either side of that interface except as specifically required by the interface definitions. Therefore the programmer's bid to simplify high-level structure translates to greater scope to segregate the resulting hardware. This is what leads to limiting the level at which computer re-creation will be applied: we can do better at the higher levels by using program structure as a guide.

Further, obviously, if a program uses multiple processes then it has in fact been written for multiple independant computers and an operating system will map it onto a single computer if necessary. Such programs are the clearest example of the kind of segregation that corresponds well to our apprach and processes, where used, represent the highest level of program decomposition (at least for programs that *can* run on a single machine). Threads may resemble processes in cases where the sharing of data is minimal and properly controlled using mutexes etc.


Summary
-------

We divide up software programs into three levels. Starting at the bottom they are: atomic, sequential and structural. At the atomic level, we transliterate individual statements or small blocks of statements into simple (combinational) logic.

The sequential level may be thought of as the implmentation within a typical class, consists of state machines, sequential code sequences and (possibly recursive) internal function calls. We device a minimally computer-like solution for each class, and instance it once per object. The extent to which we add computer-like infrastructure depends on the complexity of the class and should never be disproportionate. This permits us to acheive sequential control and algorithmic processing with the full expressivity of a software language but it is not particularly parallelisable.

The structural level deals with the allocation of objects, object hierarchy and interactions between objects. For this purpose we regard objects as autonomous interacting agents and interactions between objects (eg inter-object member funciton calls) become interactions over a protocol. This approach adds bulk in duplicated functionality and protocol support infrastructure, but has improved parallelism capability and arguably matches the conceptual structure of the original program.

The distinction between these approaches will complicate the process of conversion. At some level, the "kink" in our transformation of functionality between the insides of objects and between objects reflects the kink that already exists in all large software systems: low-level implementation details are "close" to the single, central processor and must be coded to make good use of it, wheras the higher levels of a program are more distant and may therefore be viewed as a more generic technology problem. Therefore in the former case we must deal with the loss of that central processor, wheras in the latter case we may convert more directly. 

Since the atomic level corresponds (roughly) directly to technologies found inside a processor but not specific to it, we may simply build the same solutions directly into the end hardware.

